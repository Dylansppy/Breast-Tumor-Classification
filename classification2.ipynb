{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to Describe Features extracted\n",
    "def describeData(X,y):\n",
    "    print('Total number of images: {}'.format(len(X)))\n",
    "    print('Number of Benign Images: {}'.format(np.sum(y==0)))\n",
    "    print('Number of Malignant Images: {}'.format(np.sum(y==1)))\n",
    "    print('Percentage of positive images: {:.2f}%'.format(100*np.mean(y)))\n",
    "    print('Image shape (Samples, Rows, Columns, Features): {}'.format(X[0].shape))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discription of features extracted from 40x images by model xception:\n",
      "Training Set\n",
      "Total number of images: 1589\n",
      "Number of Benign Images: 485\n",
      "Number of Malignant Images: 1104\n",
      "Percentage of positive images: 69.48%\n",
      "Image shape (Samples, Rows, Columns, Features): (1, 10, 10, 2048)\n",
      "\n",
      "Testing Set\n",
      "Total number of images: 406\n",
      "Number of Benign Images: 140\n",
      "Number of Malignant Images: 266\n",
      "Percentage of positive images: 65.52%\n",
      "Image shape (Samples, Rows, Columns, Features): (1, 10, 10, 2048)\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "Discription of features extracted from 40x images by model vgg16:\n",
      "Training Set\n",
      "Total number of images: 1589\n",
      "Number of Benign Images: 485\n",
      "Number of Malignant Images: 1104\n",
      "Percentage of positive images: 69.48%\n",
      "Image shape (Samples, Rows, Columns, Features): (1, 7, 7, 512)\n",
      "\n",
      "Testing Set\n",
      "Total number of images: 406\n",
      "Number of Benign Images: 140\n",
      "Number of Malignant Images: 266\n",
      "Percentage of positive images: 65.52%\n",
      "Image shape (Samples, Rows, Columns, Features): (1, 7, 7, 512)\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "Discription of features extracted from 40x images by model vgg19:\n",
      "Training Set\n",
      "Total number of images: 1589\n",
      "Number of Benign Images: 485\n",
      "Number of Malignant Images: 1104\n",
      "Percentage of positive images: 69.48%\n",
      "Image shape (Samples, Rows, Columns, Features): (1, 14, 14, 512)\n",
      "\n",
      "Testing Set\n",
      "Total number of images: 406\n",
      "Number of Benign Images: 140\n",
      "Number of Malignant Images: 266\n",
      "Percentage of positive images: 65.52%\n",
      "Image shape (Samples, Rows, Columns, Features): (1, 14, 14, 512)\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "Discription of features extracted from 100x images by model xception:\n",
      "Training Set\n",
      "Total number of images: 1635\n",
      "Number of Benign Images: 496\n",
      "Number of Malignant Images: 1139\n",
      "Percentage of positive images: 69.66%\n",
      "Image shape (Samples, Rows, Columns, Features): (1, 10, 10, 2048)\n",
      "\n",
      "Testing Set\n",
      "Total number of images: 446\n",
      "Number of Benign Images: 148\n",
      "Number of Malignant Images: 298\n",
      "Percentage of positive images: 66.82%\n",
      "Image shape (Samples, Rows, Columns, Features): (1, 10, 10, 2048)\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "Discription of features extracted from 100x images by model vgg16:\n",
      "Training Set\n",
      "Total number of images: 1635\n",
      "Number of Benign Images: 496\n",
      "Number of Malignant Images: 1139\n",
      "Percentage of positive images: 69.66%\n",
      "Image shape (Samples, Rows, Columns, Features): (1, 7, 7, 512)\n",
      "\n",
      "Testing Set\n",
      "Total number of images: 446\n",
      "Number of Benign Images: 148\n",
      "Number of Malignant Images: 298\n",
      "Percentage of positive images: 66.82%\n",
      "Image shape (Samples, Rows, Columns, Features): (1, 7, 7, 512)\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "Discription of features extracted from 100x images by model vgg19:\n",
      "Training Set\n",
      "Total number of images: 1635\n",
      "Number of Benign Images: 496\n",
      "Number of Malignant Images: 1139\n",
      "Percentage of positive images: 69.66%\n",
      "Image shape (Samples, Rows, Columns, Features): (1, 14, 14, 512)\n",
      "\n",
      "Testing Set\n",
      "Total number of images: 446\n",
      "Number of Benign Images: 148\n",
      "Number of Malignant Images: 298\n",
      "Percentage of positive images: 66.82%\n",
      "Image shape (Samples, Rows, Columns, Features): (1, 14, 14, 512)\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "Discription of features extracted from 200x images by model xception:\n",
      "Training Set\n",
      "Total number of images: 1574\n",
      "Number of Benign Images: 476\n",
      "Number of Malignant Images: 1098\n",
      "Percentage of positive images: 69.76%\n",
      "Image shape (Samples, Rows, Columns, Features): (1, 10, 10, 2048)\n",
      "\n",
      "Testing Set\n",
      "Total number of images: 439\n",
      "Number of Benign Images: 147\n",
      "Number of Malignant Images: 292\n",
      "Percentage of positive images: 66.51%\n",
      "Image shape (Samples, Rows, Columns, Features): (1, 10, 10, 2048)\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "Discription of features extracted from 200x images by model vgg16:\n",
      "Training Set\n",
      "Total number of images: 1574\n",
      "Number of Benign Images: 476\n",
      "Number of Malignant Images: 1098\n",
      "Percentage of positive images: 69.76%\n",
      "Image shape (Samples, Rows, Columns, Features): (1, 7, 7, 512)\n",
      "\n",
      "Testing Set\n",
      "Total number of images: 439\n",
      "Number of Benign Images: 147\n",
      "Number of Malignant Images: 292\n",
      "Percentage of positive images: 66.51%\n",
      "Image shape (Samples, Rows, Columns, Features): (1, 7, 7, 512)\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "Discription of features extracted from 200x images by model vgg19:\n",
      "Training Set\n",
      "Total number of images: 1574\n",
      "Number of Benign Images: 476\n",
      "Number of Malignant Images: 1098\n",
      "Percentage of positive images: 69.76%\n",
      "Image shape (Samples, Rows, Columns, Features): (1, 14, 14, 512)\n",
      "\n",
      "Testing Set\n",
      "Total number of images: 439\n",
      "Number of Benign Images: 147\n",
      "Number of Malignant Images: 292\n",
      "Percentage of positive images: 66.51%\n",
      "Image shape (Samples, Rows, Columns, Features): (1, 14, 14, 512)\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "Discription of features extracted from 400x images by model xception:\n",
      "Training Set\n",
      "Total number of images: 1432\n",
      "Number of Benign Images: 458\n",
      "Number of Malignant Images: 974\n",
      "Percentage of positive images: 68.02%\n",
      "Image shape (Samples, Rows, Columns, Features): (1, 10, 10, 2048)\n",
      "\n",
      "Testing Set\n",
      "Total number of images: 388\n",
      "Number of Benign Images: 130\n",
      "Number of Malignant Images: 258\n",
      "Percentage of positive images: 66.49%\n",
      "Image shape (Samples, Rows, Columns, Features): (1, 10, 10, 2048)\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "Discription of features extracted from 400x images by model vgg16:\n",
      "Training Set\n",
      "Total number of images: 1432\n",
      "Number of Benign Images: 458\n",
      "Number of Malignant Images: 974\n",
      "Percentage of positive images: 68.02%\n",
      "Image shape (Samples, Rows, Columns, Features): (1, 7, 7, 512)\n",
      "\n",
      "Testing Set\n",
      "Total number of images: 388\n",
      "Number of Benign Images: 130\n",
      "Number of Malignant Images: 258\n",
      "Percentage of positive images: 66.49%\n",
      "Image shape (Samples, Rows, Columns, Features): (1, 7, 7, 512)\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "Discription of features extracted from 400x images by model vgg19:\n",
      "Training Set\n",
      "Total number of images: 1432\n",
      "Number of Benign Images: 458\n",
      "Number of Malignant Images: 974\n",
      "Percentage of positive images: 68.02%\n",
      "Image shape (Samples, Rows, Columns, Features): (1, 14, 14, 512)\n",
      "\n",
      "Testing Set\n",
      "Total number of images: 388\n",
      "Number of Benign Images: 130\n",
      "Number of Malignant Images: 258\n",
      "Percentage of positive images: 66.49%\n",
      "Image shape (Samples, Rows, Columns, Features): (1, 14, 14, 512)\n",
      "\n",
      "-----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Spliting the features into training and testing set at 80%/20% ratio\n",
    "from sklearn.model_selection import GroupShuffleSplit     \n",
    "\n",
    "def train_test_group_split(X, y, p):\n",
    "    gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=2)\n",
    "    for train_index, test_index in gss.split(X, y, p):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        p_train, p_test = p[train_index], p[test_index]\n",
    "    return X_train, y_train, X_test, y_test, p_train, p_test\n",
    "\n",
    "def feature_dimension(model, factor):\n",
    "    X = np.load('../data/features_' + model + '/' + str(factor)+'/X.npy')\n",
    "    y = np.load('../data/features_' + model + '/' + str(factor)+'/y.npy')\n",
    "    p = np.load('../data/features_' + model + '/' + str(factor)+'/p.npy')\n",
    "    print('Discription of features extracted from ' + str(factor) + 'x images by model ' + model + \":\")\n",
    "    X_train, y_train, X_test, y_test, p_train, p_test = train_test_group_split(X, y, p)\n",
    "    print('Training Set')\n",
    "    describeData(X_train, y_train)\n",
    "    print('Testing Set')\n",
    "    describeData(X_test, y_test)\n",
    "    print(\"-----------------------------------------------------------------------\")\n",
    "    \n",
    "# Showing the features dimension after train_test_split\n",
    "magnification_factors = ['40', '100', '200', '400']\n",
    "models_list = ['xception','vgg16','vgg19']\n",
    "for factor in magnification_factors:\n",
    "    for model in models_list:\n",
    "        feature_dimension(model, factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance of pre-trained CNN model xception for images at 40x:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Benign       0.72      0.65      0.68       140\n",
      "   Malignant       0.82      0.86      0.84       266\n",
      "\n",
      "   micro avg       0.79      0.79      0.79       406\n",
      "   macro avg       0.77      0.76      0.76       406\n",
      "weighted avg       0.79      0.79      0.79       406\n",
      "\n",
      "F1 Score: 0.8440366972477065\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'metrics' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-21d85320e9bd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfactor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmagnification_factors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m     \u001b[0mModel_Performance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'xception'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfactor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m     \u001b[0mModel_Performance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'vgg16'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfactor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mModel_Performance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'vgg19'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfactor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-21d85320e9bd>\u001b[0m in \u001b[0;36mModel_Performance\u001b[0;34m(model, magnification_factor)\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Performance of pre-trained CNN model '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' for images at '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmagnification_factor\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'x:'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mclassifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msolver\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'liblinear'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_trainFlat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     \u001b[0mtest_performance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_testFlat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfactor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmagnification_factors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-21d85320e9bd>\u001b[0m in \u001b[0;36mtest_performance\u001b[0;34m(X_test, y_test, model)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m# ROC curve\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0my_predict_proba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mfpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroc_curve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_predict_proba\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0mroc_auc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtpr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'metrics' is not defined"
     ]
    }
   ],
   "source": [
    "# Classification Performance\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "# Test Model Performance on Testing Set\n",
    "def test_performance(X_test, y_test, model):\n",
    "    # Classification report\n",
    "    labels = [\"Benign\", \"Malignant\"]\n",
    "    y_predict = model.predict(X_test)\n",
    "    print(classification_report(y_test, y_predict, target_names=labels))\n",
    "    print(\"F1 Score: {}\".format(f1_score(y_test, y_predict)))\n",
    "    # ROC curve\n",
    "    y_predict_proba = model.predict_proba(X_test)\n",
    "    fpr, tpr, _ = metrics.roc_curve(y_test, y_predict_proba[:,1])\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "    plt.figure()\n",
    "    lw = 2\n",
    "    plt.plot(fpr, tpr, color='darkorange',\n",
    "           lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic example')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "# Function returning image level pre_trained model performance\n",
    "def Model_Performance(model, magnification_factor):\n",
    "    # Make Features from Training Set 1D for compatability with standard classifiers\n",
    "    X=np.load('../data/features_' + model + '/' + magnification_factor +'/X.npy')\n",
    "    X_Shape = X.shape[1]*X.shape[2]*X.shape[3]*X.shape[4]\n",
    "    X_Flat = X.reshape(X.shape[0], X_Shape)\n",
    "    y = np.load('../data/features_' + model  + '/' + magnification_factor+'/y.npy')\n",
    "    p = np.load('../data/features_' + model  + '/' + magnification_factor+'/p.npy')\n",
    "    # Split into training and testing set with defined\n",
    "    X_trainFlat, y_train, X_testFlat, y_test, p_train, p_test = train_test_group_split(X_Flat, y, p)\n",
    "    # Hyperparameter tuning\n",
    "    # optimized_classifier(X_trainFlat, y_train, p_train, 'LR' )\n",
    "    # classifier = optimized_classifier(X_trainFlat, y_train, p_train, 'SVM')\n",
    "    # Classification Performance on testing data\n",
    "    print('Performance of pre-trained CNN model ' + model + ' for images at ' + magnification_factor + 'x:')\n",
    "    classifier = LogisticRegression(solver='liblinear').fit(X_trainFlat, y_train)\n",
    "    test_performance(X_testFlat, y_test, classifier)\n",
    "    \n",
    "for factor in magnification_factors:\n",
    "    Model_Performance('xception', factor)\n",
    "    Model_Performance('vgg16', factor)\n",
    "    Model_Performance('vgg19', factor)\n",
    "    print(\"-------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Patient level accuracy    \n",
    "def patient_level_accuracy(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"Run LogisticRegression as Classifier\"\"\"   \n",
    "    clf = model.fit(X_train, y_train)\n",
    "    preds = clf.predict(X_test)\n",
    "    print(classification_report(y_test, preds, target_names=labels))\n",
    "    print(\"F1 Score: {}\".format(f1_score(d, preds)))\n",
    "    #print(\"acc: %.02f\" % accuracy_score(d, preds))\n",
    "    #kfold = model_selection.KFold(n_splits=10)\n",
    "    #f1= model_selection.cross_val_score(model, c,d, cv=kfold, scoring='f1')\n",
    "    #mean = f1.mean() \n",
    "    #stdev = f1.std()\n",
    "    #print('F1 score: %s (%s)' % (mean, stdev))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different Candidate Classification Algorithms\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "SVM_parameters = {'kernel':('linear', 'rbf'), \n",
    "                  'C':(1, 10)}\n",
    "\n",
    "LR_parameters = {'penalty': ('l1', 'l2'),\n",
    "                 'C' : (0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000),\n",
    "                 'class_weight' : ({1:0.5, 0:0.5}, {1:0.4, 0:0.6}, {1:0.6, 0:0.4}),\n",
    "                 'solver' : ('liblinear', 'saga')}\n",
    "\n",
    "classifiers = {'LR': (LogisticRegression(), LR_parameters),\n",
    "               'LDA': LinearDiscriminantAnalysis(),\n",
    "               'DTC': DecisionTreeClassifier(),\n",
    "               'RF': RandomForestClassifier(),\n",
    "               'GBC': GradientBoostingClassifier(),\n",
    "               'KNN': KNeighborsClassifier(),\n",
    "               'SVM': (SVC(), SVM_parameters),\n",
    "               'LSVM': LinearSVC(),\n",
    "               'GNB': GaussianNB()}\n",
    "                      \n",
    "\n",
    "# Tuning hyperparameter to get optimized classifier using GridSearchCV\n",
    "def optimized_classifier(X_train, y_train, p_train, classifier):\n",
    "    \"\"\"Function that use logistic regression as classifier and return Cross-validated F1 Score\"\"\"\n",
    "    # Hyperparameter Tuning using group k folds cross validation\n",
    "    group_kfold = GroupKFold(n_splits=5)\n",
    "    parameters = classifiers[classifier][1]\n",
    "    model = classifiers[classifier][0]\n",
    "    grid = GridSearchCV(estimator=model, \n",
    "                       param_grid=parameters,\n",
    "                       cv=group_kfold,\n",
    "                       scoring=['roc_auc','f1'],\n",
    "                       verbose=1,\n",
    "                       n_jobs=-1,\n",
    "                       refit='f1')\n",
    "    grid_result = grid.fit(X_train, y_train, p_train) \n",
    "    # summarize results\n",
    "    print(\"Best F1 score: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "    means = grid_result.cv_results_['mean_test_score']\n",
    "    stds = grid_result.cv_results_['std_test_score']\n",
    "    params = grid_result.cv_results_['params']\n",
    "    for mean, stdev, param in zip(means, stds, params):\n",
    "        print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "    return grid_result.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuning hyperparameter to get optimized classifier using RandomizedSearchCV\n",
    "loss = ['hinge', 'log', 'modified_huber', 'squared_hinge', 'perceptron']\n",
    "penalty = ['l1', 'l2', 'elasticnet']\n",
    "alpha = [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "learning_rate = ['constant', 'optimal', 'invscaling', 'adaptive']\n",
    "class_weight = [{1:0.5, 0:0.5}, {1:0.4, 0:0.6}, {1:0.6, 0:0.4}, {1:0.7, 0:0.3}]\n",
    "eta0 = [1, 10, 100]\n",
    "\n",
    "param_distributions = dict(loss=loss,\n",
    "                           penalty=penalty,\n",
    "                           alpha=alpha,\n",
    "                           learning_rate=learning_rate,\n",
    "                           class_weight=class_weight,\n",
    "                           eta0=eta0)\n",
    "\n",
    "random = RandomizedSearchCV(estimator=sgd,\n",
    "                            param_distributions=param_distributions,\n",
    "                            scoring='roc_auc',\n",
    "                            verbose=1, n_jobs=-1,\n",
    "                            n_iter=1000)\n",
    "random_result = random.fit(X_train, y_train)\n",
    "\n",
    "print('Best Score: ', random_result.best_score_)\n",
    "print('Best Params: ', random_result.best_params_)\n",
    "\n",
    "# Using Logistic Regression as Classifier\n",
    "def runLogisticRegression(a,b,c,d):\n",
    "    \"\"\"Run LogisticRegression as Classifier\"\"\"\n",
    "    model = LogisticRegression(solver='liblinear')\n",
    "    clf = model.fit(a,b)\n",
    "    preds = clf.predict(c)\n",
    "    print(classification_report(d, preds, target_names=labels))\n",
    "    print(\"F1 Score: {}\".format(f1_score(d, preds)))\n",
    "    #print(\"acc: %.02f\" % accuracy_score(d, preds))\n",
    "    #kfold = model_selection.KFold(n_splits=10)\n",
    "    #mean = f1.mean() \n",
    "    #stdev = f1.std()\n",
    "    #print('F1 score: %s (%s)' % (mean, stdev))\n",
    "    print('')\n",
    "    \n",
    "# Test the Function\n",
    "X = np.load('../data/features_xception/40/X.npy')\n",
    "X_Shape = X.shape[1]*X.shape[2]*X.shape[3]*X.shape[4]\n",
    "X_Flat = X.reshape(X.shape[0], X_Shape)\n",
    "y = np.load('../data/features_xception/40/y.npy')\n",
    "p = np.load('../data/features_xception/40/p.npy')\n",
    "optimized_classifier(X_Flat, y, p, 'LR' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def defineClassifiers():\n",
    "    \"\"\"\n",
    "    This function just defines each abbreviation used in the previous function (e.g. LR = Logistic Regression)\n",
    "    \"\"\"\n",
    "    print('')\n",
    "    print('LR = LogisticRegression')\n",
    "    print('RF = RandomForestClassifier')\n",
    "    print('KNN = KNeighborsClassifier')\n",
    "    print('SVM = Support Vector Machine SVC')\n",
    "    print('LSVM = LinearSVC')\n",
    "    print('GNB = GaussianNB')\n",
    "    print('DTC = DecisionTreeClassifier')\n",
    "    #print('GBC = GradientBoostingClassifier')\n",
    "    #print('LDA = LinearDiscriminantAnalysis')\n",
    "    print('')\n",
    "    return\n",
    "defineClassifiers()\n",
    "    \n",
    "def compareABunchOfDifferentModelsAccuracy(a,b,c,d):\n",
    "    \"\"\"\n",
    "    compare performance of classifiers on X_train, X_test, Y_train, Y_test\n",
    "    http://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html#sklearn.metrics.accuracy_score\n",
    "    http://scikit-learn.org/stable/modules/model_evaluation.html#accuracy-score\n",
    "    \"\"\"    \n",
    "    print('')\n",
    "    print('Compare Multiple Classifiers:')\n",
    "    print('')\n",
    "    print('K-Fold Cross-Validation Accuracy:')\n",
    "    print('')      \n",
    "    resultsAccuracy = []\n",
    "    names = []\n",
    "    for name, model in classifiers:\n",
    "        model.fit(a, b)\n",
    "        kfold = model_selection.KFold(n_splits=10)\n",
    "        accuracy_results = model_selection.cross_val_score(model, c, d, cv=kfold, scoring='accuracy')\n",
    "        resultsAccuracy.append(accuracy_results)\n",
    "        names.append(name)\n",
    "        accuracyMessage = \"%s: %f (%f)\" % (name, accuracy_results.mean(), accuracy_results.std())\n",
    "        print(accuracyMessage)  \n",
    "    # boxplot algorithm comparison\n",
    "    fig = plt.figure()\n",
    "    fig.suptitle('Algorithm Comparison: Accuracy')\n",
    "    ax = fig.add_subplot(111)\n",
    "    plt.boxplot(resultsAccuracy)\n",
    "    ax.set_xticklabels(names)\n",
    "    ax.set_ylabel('Cross-Validation: Accuracy Score')\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "compareABunchOfDifferentModelsAccuracy(X_trainFlat, Y_train, X_testFlat, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(X)):\n",
    "    x=GlobalAveragePooling2D()(x)\n",
    "    x=Dense(1024,activation='relu')(x) #we add dense layers so that the model can learn more complex functions and classify for better results.\n",
    "    x=Dense(1024,activation='relu')(x) #dense layer 2\n",
    "    x=Dense(512,activation='relu')(x) #dense layer 3\n",
    "    preds=Dense(2,activation='softmax')(x) #final layer with softmax activation\n",
    "\n",
    "def classification_training(train_or_test, feature_extractor, factor):\n",
    "    X = np.load('../data/features_' + model + '/'+ train_or_test + '/' + str(factor)+'/X.npy')\n",
    "    y = np.load('../data/features_' + model + '/'+ train_or_test + '/' + str(factor)+'/y.npy')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
